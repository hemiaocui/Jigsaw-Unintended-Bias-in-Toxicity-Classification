{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import re\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_tr = pd.read_csv('train.csv')\n",
    "dat_te = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all 'comment_text' values are string\n",
    "dat_tr[\"comment_text\"] = dat_tr[\"comment_text\"].astype(str)\n",
    "dat_te[\"comment_text\"] = dat_te[\"comment_text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dat_tr ,dat_te], sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_glove = load_embed('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_paragram = load_embed('paragram_300_sl999.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build vocab and check coverage of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key = operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 15.519% of vocab\n",
      "Found embeddings for 89.608% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_glove = check_coverage(vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 8.796% of vocab\n",
      "Found embeddings for 78.632% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_paragram = check_coverage(vocab, embed_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that only 15.519% of the data has embedding in GloVe, and only 8.8% has embedding in paragram. The second line implies that about 10% of the data is useless if GloVe is used, and about 20% if paragram is used. This needs to be improved. First, we need to see some OOV (out of vocabulary) word in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"isn't\", 42192),\n",
       " (\"That's\", 39614),\n",
       " (\"won't\", 31075),\n",
       " (\"he's\", 25672),\n",
       " (\"Trump's\", 24673),\n",
       " (\"aren't\", 21696),\n",
       " (\"wouldn't\", 20611),\n",
       " ('Yes,', 20040),\n",
       " ('that,', 19210),\n",
       " (\"wasn't\", 19084)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_glove[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 908830),\n",
       " ('The', 459088),\n",
       " ('Trump', 165619),\n",
       " ('It', 162342),\n",
       " ('You', 152288),\n",
       " ('If', 151757),\n",
       " ('And', 134926),\n",
       " ('This', 128001),\n",
       " ('They', 106274),\n",
       " ('We', 96380)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_paragram[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some problems found here is that the vocab has punctuation (e.g. comma \",\"), contractions (e.g. \"isn't\") not found in the GloVE. For paragram, the main problem is with capital/lower lettter. In the next step, some manipulations will be done to increase the coverage of the embeddings. This will include lowering capital letters, eliminating contractions, eliminating punctuation, spelling correction, and number elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text manipulation to increase coverage of the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lowering capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowering all capital, add it to GloVe if it's not there\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 25061 words to embedding\n",
      "Found embeddings for 15.638% of vocab\n",
      "Found embeddings for 89.637% of all text\n"
     ]
    }
   ],
   "source": [
    "add_lower(embed_glove, vocab)\n",
    "oov_glove = check_coverage(vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 words to embedding\n",
      "Found embeddings for 8.796% of vocab\n",
      "Found embeddings for 78.632% of all text\n"
     ]
    }
   ],
   "source": [
    "add_lower(embed_paragram, vocab)\n",
    "oov_paragram = check_coverage(vocab, embed_paragram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lowered_comment'] = df['comment_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Eliminating contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean contractions based on the map above\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"‚Äô\", \"‚Äò\", \"¬¥\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['treated_comment'] = df['lowered_comment'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['treated_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 13.506% of vocab\n",
      "Found embeddings for 90.394% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_glove = check_coverage(vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 13.699% of vocab\n",
      "Found embeddings for 90.399% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_paragram = check_coverage(vocab, embed_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coverage of paragram embedding is now as good as GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Eliminating punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"‚Äú‚Äù‚Äô' + '‚àûŒ∏√∑Œ±‚Ä¢√†‚àíŒ≤‚àÖ¬≥œÄ‚Äò‚Çπ¬¥¬∞¬£‚Ç¨\\√ó‚Ñ¢‚àö¬≤‚Äî‚Äì&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_mapping = {\"‚Äò\": \"'\", \"‚Çπ\": \"e\", \"¬¥\": \"'\", \"¬∞\": \"\", \"‚Ç¨\": \"e\", \"‚Ñ¢\": \"tm\", \"‚àö\": \" sqrt \", \"√ó\": \"x\", \"¬≤\": \"2\", \"‚Äî\": \"-\", \"‚Äì\": \"-\", \"‚Äô\": \"'\", \"_\": \"-\", \"`\": \"'\", '‚Äú': '\"', '‚Äù': '\"', '‚Äú': '\"', \"¬£\": \"e\", '‚àû': 'infinity', 'Œ∏': 'theta', '√∑': '/', 'Œ±': 'alpha', '‚Ä¢': '.', '√†': 'a', '‚àí': '-', 'Œ≤': 'beta', '‚àÖ': '', '¬≥': '3', 'œÄ': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '‚Ä¶': ' ... ', '\\ufeff': '', '‡§ï‡§∞‡§®‡§æ': '', '‡§π‡•à': ''}\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['treated_comment'] = df['treated_comment'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['treated_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 54.207% of vocab\n",
      "Found embeddings for 99.723% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_glove = check_coverage(vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 57.633% of vocab\n",
      "Found embeddings for 99.739% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_paragram = check_coverage(vocab, embed_paragram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brexit', 2043),\n",
       " ('theglobeandmail', 1423),\n",
       " ('qu√©bec', 1365),\n",
       " ('drumpf', 1294),\n",
       " ('deplorables', 1274),\n",
       " ('trumpcare', 869),\n",
       " ('sb91', 841),\n",
       " ('theguardian', 795),\n",
       " ('klastri', 754),\n",
       " ('trumpism', 599),\n",
       " ('·¥Ä', 551),\n",
       " ('‚ú∞', 550),\n",
       " ('·¥Ä…¥·¥Ö', 540),\n",
       " ('‚Äï', 507),\n",
       " ('auwe', 500),\n",
       " ('na√Øve', 497),\n",
       " ('¬ª', 469),\n",
       " ('¬´', 454),\n",
       " ('trumpsters', 416),\n",
       " ('trumpian', 410)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_glove[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Misspells and other ambiguous texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still a lot of OOV words. Most of them are related to politics (Trump, Hillary, Obama, Justin Trudeau) and news platform. Here, an attempt is made to translate these OOVs manually. It should be noted that this is not an exclusive list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {'theglobeandmail': 'news', 'theguardian':'news', 'qu√©bec':'quebec', 'drumpf':'trump', 'trumpcare': 'trump', 'trumpism':'trump', 'trumpian':'trump', 'trumpsters':'trump', '·¥Ä…¥·¥Ö': 'and', 'na√Øve': 'naive', 'qu√©b√©cois': 'quebecois', '·¥õ ú·¥á': 'the', 'montr√©al': 'montreal', ' ú·¥è·¥ç·¥á': 'home', '·¥ú·¥ò': 'up', ' ô è': 'by', 'y·¥è·¥ú': 'you', '·¥Ä·¥õ': 'at', 'koncerned': 'concerned', 'thedonald': 'trump', 'trumpkins':'trump', 'washingtontimes': 'news', '·¥Ñ ú·¥á·¥Ñ·¥ã': 'check', '“ì·¥è Ä': 'for', '·¥Ñ·¥è·¥ç·¥ò·¥ú·¥õ·¥á Ä': 'computer', '·¥õ ú…™s': 'this', '·¥ç·¥è…¥·¥õ ú': 'month', '·¥°·¥è Ä·¥ã…™…¥…¢': 'working', 'chr√©tien': 'chretien', '·¥ä·¥è ô':'job', '·¥è“ì':'of', ' ú·¥è·¥ú Ä ü è':'hourly', '·¥°·¥á·¥á·¥ã':'week', ' ü…™…¥·¥ã':'link', '·¥õ·¥è':'to', ' ú·¥Ä·¥†·¥á':'have', '·¥Ñ·¥Ä…¥':'can', '·¥á…¥·¥Ö':'end', 'üòÄ':'smiley', 'üòÇ':'laugh', 'üòâ':'wink', 'trumpies':'trump', 'trumpty':'trump', 'trumpettes':'trump', 'üòÉ':'smiley', 'üòä':'smiley', 'torontosun':'news', 'vancouversun':'news', 'theintercept':'news', 'realdonaldtrump':'trump', 'trumpland':'trump', 'drump':'trump', 'trumpnuts':'trump', 'trumpo':'trump', 'nationalobserver': 'news', 'thefederalist':'news', 'trumpanzees':'trump', 'trumpski':'trump', 'hawai ªi':'hawaii', 'trumpites':'trump', 'trumpie':'trump', 'americamagazine':'news', 'thecanadianencyclopedia':'news', 'trumpians':'trump', 'trumptards':'trump', 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pok√©mon': 'pokemon'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['treated_comment'] = df['treated_comment'].apply(lambda x: correct_spelling(x, mispell_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['treated_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 54.223% of vocab\n",
      "Found embeddings for 99.739% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_glove = check_coverage(vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 57.651% of vocab\n",
      "Found embeddings for 99.754% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_paragram = check_coverage(vocab, embed_paragram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['treated_comment'] = df['treated_comment'].apply(lambda x: clean_numbers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the steps above for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train data\n",
    "dat_tr['treated_comment'] = dat_tr['comment_text'].apply(lambda x: x.lower())\n",
    "dat_tr['treated_comment'] = dat_tr['treated_comment'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "dat_tr['treated_comment'] = dat_tr['treated_comment'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "dat_tr['treated_comment'] = dat_tr['treated_comment'].apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "dat_tr['treated_comment'] = dat_tr['treated_comment'].apply(lambda x: clean_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test data\n",
    "dat_te['treated_comment'] = dat_te['comment_text'].apply(lambda x: x.lower())\n",
    "dat_te['treated_comment'] = dat_te['treated_comment'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "dat_te['treated_comment'] = dat_te['treated_comment'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "dat_te['treated_comment'] = dat_te['treated_comment'].apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "dat_te['treated_comment'] = dat_te['treated_comment'].apply(lambda x: clean_numbers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the weighted average between the two embedding matrices will be used. That is 0.7 weight for GloVe and 0.3 weight for paragram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2221077"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703756"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_paragram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_meta(embed1,embed2,weight1,weight2):\n",
    "    meta_embed={}\n",
    "    word_union=set(embed1.keys()).union(set(embed2.keys()))\n",
    "    word_only_in_embed1=set(embed1.keys())-set(embed2.keys())\n",
    "    word_only_in_embed2=set(embed2.keys())-set(embed1.keys())\n",
    "    for word in word_union:\n",
    "        if word in word_only_in_embed1: \n",
    "            meta_embed[word]=embed1[word]\n",
    "        elif word in word_only_in_embed2: \n",
    "            meta_embed[word]=embed2[word]\n",
    "        else: \n",
    "            meta_embed[word]=embed1[word]*weight1+embed2[word]*weight2           \n",
    "    return meta_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_embed=build_meta(embed_glove,embed_paragram,0.7,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2912339"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob polarity value will be added for sentiment analysis of the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_te['polarity'] = dat_te['treated_comment'].apply(lambda comment: TextBlob(comment).polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_tr['polarity'] = dat_tr['treated_comment'].apply(lambda comment: TextBlob(comment).polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of words; Number of unique words; Textblob word subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_te['subjectivity'] = dat_te['treated_comment'].apply(lambda comment: TextBlob(comment).subjectivity)\n",
    "dat_tr['subjectivity'] = dat_tr['treated_comment'].apply(lambda comment: TextBlob(comment).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_te['unique_word_count']=dat_te['treated_comment'].apply(lambda x:len(set(x.split())))\n",
    "dat_tr['unique_word_count']=dat_tr['treated_comment'].apply(lambda x:len(set(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_te['total_word_count']=dat_te['treated_comment'].apply(lambda x:len(x.split()))\n",
    "dat_tr['total_word_count']=dat_tr['treated_comment'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting dataset and meta-embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data\n",
    "dat_tr.to_csv(\"train_treated.csv\")\n",
    "dat_te.to_csv(\"test_treated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving meta embedding pickle file\n",
    "with open('meta_embed.pickle', 'wb') as handle:\n",
    "    pickle.dump(meta_embed, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove.pickle', 'wb') as handle:\n",
    "    pickle.dump(embed_glove, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embed_paragram.pickle', 'wb') as handle:\n",
    "    pickle.dump(embed_paragram, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
